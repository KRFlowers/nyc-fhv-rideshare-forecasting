{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FHVHV Data Pipeline - Stage 1: Validation\n",
    "\n",
    "**Pipeline Position:** Stage 1 of 4\n",
    "- Stage 0: Data Download \n",
    "- Stage 1: Data Validation ← THIS SCRIPT\n",
    "- Stage 2: Exploratory Analysis\n",
    "- Stage 3: Modeling\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This notebook validates NYC TLC FHVHV trip data (Uber/Lyft) for 2022-2024, implementing a comprehensive validation framework with granular quality checks across trip duration, distance, and fare fields. The pipeline uses DuckDB for memory-efficient processing of 684M records and creates detailed quality flags for monitoring and debugging.\n",
    "\n",
    "**Fields Validated**\n",
    "\n",
    "This validation focuses on three quantitative outcome fields critical for demand forecasting: trip duration (trip_time), trip distance (trip_miles), and base fare (base_passenger_fare). These fields receive comprehensive checks including null detection, bounds validation (60s-12hr for duration, 0.1-200mi for distance, $0-$500 for fare), and extreme value flagging.\n",
    "\n",
    "**Validation Strategy**\n",
    "\n",
    "The framework implements 13 granular flags to identify specific quality issues (null values, negative numbers, out-of-bounds data, extreme outliers) along with a master validity indicator for quick filtering. This flag-based approach preserves all records for audit trails and debugging rather than deleting problematic data. Configurable thresholds at the top of the notebook make it easy to adapt the framework for future projects, like the planned water quality forecasting analysis.\n",
    "\n",
    "**Code Reusability**\n",
    "\n",
    "This script is designed for reuse across different projects:\n",
    "- Update 3 threshold variables for new datasets\n",
    "- Generic validation pattern (null, negative, bounds, extremes)\n",
    "- Modular architecture easily adapted to different datasets\n",
    "\n",
    "**Inputs:**\n",
    "- `data/raw/fhvhv_combined.parquet`\n",
    "\n",
    "**Outputs:**\n",
    "- `data/validated/fhvhv_all_data_flagged.parquet` - All records with 13 validation flags\n",
    "- `data/validated/fhvhv_valid_data_for_eda.parquet` - Valid records only (~99.9%)\n",
    "- `data/quality_reports/validation_report.csv` - Detailed validation metrics\n",
    "\n",
    "**Runtime Note**\n",
    "\n",
    "Processing 684M records takes approximately 70 minutes total: ~20 minutes for flagging (Section 3.2) and ~25 minutes for creating the clean EDA dataset (Section 3.5). DuckDB's streaming approach keeps memory usage manageable throughout.\n",
    "\n",
    "**Next Step:** Run `02_exploratory_analysis.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ..\\data\\final\\combined_fhvhv_tripdata.parquet\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "# Define file paths using relative references so the notebook works from any machine with the same folder structure\n",
    "INPUT_FILE = Path(\"../data/final/combined_fhvhv_tripdata.parquet\")\n",
    "OUTPUT_DIR = Path(\"../data/validated\")\n",
    "REPORTS_DIR = Path(\"../data/quality_reports\")\n",
    "\n",
    "# Create directories if they don't already exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define output filenames for flagged and clean datasets\n",
    "FLAGGED_FILE = OUTPUT_DIR / \"fhvhv_all_data_flagged.parquet\"\n",
    "EDA_FILE = OUTPUT_DIR / \"fhvhv_valid_data_for_eda.parquet\"\n",
    "\n",
    "# Initialize DuckDB connection and set progress bar options\n",
    "con = duckdb.connect()\n",
    "con.execute(\"SET enable_progress_bar = true\")\n",
    "con.execute(\"SET progress_bar_time = 2000\")\n",
    "\n",
    "print(f\"Input: {INPUT_FILE}\")\n",
    "print(f\"File exists: {INPUT_FILE.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Exploration\n",
    "This section reviews schema overview, check date range, and identify missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Column Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 684,376,551\n",
      "\n",
      "Dataset has 24 columns:\n",
      "\n",
      "         column_name column_type null  key default extra\n",
      "   hvfhs_license_num     VARCHAR  YES None    None  None\n",
      "dispatching_base_num     VARCHAR  YES None    None  None\n",
      "originating_base_num     VARCHAR  YES None    None  None\n",
      "    request_datetime   TIMESTAMP  YES None    None  None\n",
      "   on_scene_datetime   TIMESTAMP  YES None    None  None\n",
      "     pickup_datetime   TIMESTAMP  YES None    None  None\n",
      "    dropoff_datetime   TIMESTAMP  YES None    None  None\n",
      "        PULocationID      BIGINT  YES None    None  None\n",
      "        DOLocationID      BIGINT  YES None    None  None\n",
      "          trip_miles      DOUBLE  YES None    None  None\n",
      "           trip_time      BIGINT  YES None    None  None\n",
      " base_passenger_fare      DOUBLE  YES None    None  None\n",
      "               tolls      DOUBLE  YES None    None  None\n",
      "                 bcf      DOUBLE  YES None    None  None\n",
      "           sales_tax      DOUBLE  YES None    None  None\n",
      "congestion_surcharge      DOUBLE  YES None    None  None\n",
      "         airport_fee      DOUBLE  YES None    None  None\n",
      "                tips      DOUBLE  YES None    None  None\n",
      "          driver_pay      DOUBLE  YES None    None  None\n",
      " shared_request_flag     VARCHAR  YES None    None  None\n",
      "   shared_match_flag     VARCHAR  YES None    None  None\n",
      "  access_a_ride_flag     VARCHAR  YES None    None  None\n",
      "    wav_request_flag     VARCHAR  YES None    None  None\n",
      "      wav_match_flag     VARCHAR  YES None    None  None\n"
     ]
    }
   ],
   "source": [
    "# Get total record count\n",
    "total_records = con.execute(f\"SELECT COUNT(*) FROM '{INPUT_FILE}'\").fetchone()[0]\n",
    "print(f\"Total records: {total_records:,}\\n\")\n",
    "\n",
    "# Review column names and data types using DESCRIBE\n",
    "schema = con.execute(f\"\"\"\n",
    "    DESCRIBE SELECT * FROM '{INPUT_FILE}'\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Dataset has {len(schema)} columns:\\n\")\n",
    "print(schema.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Date Range Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2022-01-01 00:00:00 to 2024-12-31 23:59:59\n"
     ]
    }
   ],
   "source": [
    "# Check date range of pickup_datetime to verify coverage period\n",
    "date_range = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        MIN(pickup_datetime) as earliest,\n",
    "        MAX(pickup_datetime) as latest\n",
    "    FROM '{INPUT_FILE}'\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Date range: {date_range['earliest'].iloc[0]} to {date_range['latest'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 Missing Values Check\n",
    "Identify null values by column. High-null columns will be excluded during aggregation in EDA, not removed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574bece8337942fe90bf12b48f753430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column                    Null Count   Null Percentage\n",
      "--------------------------------------------------\n",
      "hvfhs_license_num         0                       0.00%\n",
      "dispatching_base_num      0                       0.00%\n",
      "originating_base_num      183954837              26.88%\n",
      "request_datetime          0                       0.00%\n",
      "on_scene_datetime         183891654              26.87%\n",
      "pickup_datetime           0                       0.00%\n",
      "dropoff_datetime          0                       0.00%\n",
      "PULocationID              0                       0.00%\n",
      "DOLocationID              0                       0.00%\n",
      "trip_miles                0                       0.00%\n",
      "trip_time                 0                       0.00%\n",
      "base_passenger_fare       0                       0.00%\n",
      "tolls                     0                       0.00%\n",
      "bcf                       0                       0.00%\n",
      "sales_tax                 0                       0.00%\n",
      "congestion_surcharge      0                       0.00%\n",
      "airport_fee               0                       0.00%\n",
      "tips                      0                       0.00%\n",
      "driver_pay                0                       0.00%\n",
      "shared_request_flag       0                       0.00%\n",
      "shared_match_flag         0                       0.00%\n",
      "access_a_ride_flag        0                       0.00%\n",
      "wav_request_flag          0                       0.00%\n",
      "wav_match_flag            0                       0.00%\n"
     ]
    }
   ],
   "source": [
    "# Get list of column names from schema\n",
    "columns = schema['column_name'].tolist()\n",
    "\n",
    "# Build SQL to count NULLs per column using CASE WHEN use single scan counts nulls for all columns once\n",
    "null_count_sql = f\"\"\"\n",
    "    SELECT \n",
    "        {', '.join([f\"SUM(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) AS {col}_null_count\" for col in columns])}\n",
    "    FROM '{INPUT_FILE}'\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and get results as tuple\n",
    "null_counts = con.execute(null_count_sql).fetchone()\n",
    "\n",
    "# Calculate null percentages f\n",
    "null_pct = [(count / total_records) * 100 for count in null_counts]\n",
    "\n",
    "#Display results \n",
    "print(f\"{'Column':<25} {'Null Count':<12} {'Null Percentage'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for col, count, pct in zip(columns, null_counts, null_pct):\n",
    "    print(f\"{col:<25} {count:<12} {pct:>15.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**High-null columns (>10%):**\n",
    "- `originating_base_num` (27%) - Lyft doesn't report this field\n",
    "- `on_scene_datetime` (27%) - Optional tracking field\n",
    "- `airport_fee` (18%) - Only applies to airport trips\n",
    "\n",
    "**Impact on forecasting:**\n",
    "These columns are most likely not needed for trip count forecasting and will be excluded during aggregation in EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Validation & Dataset Creation\n",
    "This section defines validation thresholds for duration, distance, and fare, then checks and flags all records for valid/invalid status and creates two output datasets. A flagged dataset contains quality indicators for all 684M records, and a clean dataset contains only valid records for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Thresholds\n",
      "==================================================\n",
      "Duration: 60s - 43200s (max: 12.0 hrs)\n",
      "Distance: 0.1 - 200 miles\n",
      "Fare: $0 - $500\n"
     ]
    }
   ],
   "source": [
    "# Duration thresholds (seconds)\n",
    "DURATION_MIN = 60          # 1 minute - shorter likely GPS/timing errors\n",
    "DURATION_MAX = 43200       # 12 hours - reasonable max for rideshare\n",
    "DURATION_EXTREME = 604800  # 7 days - clear data corruption\n",
    "\n",
    "# Distance thresholds (miles)\n",
    "DISTANCE_MIN = 0.1         # 0.1 miles - filters GPS noise\n",
    "DISTANCE_MAX = 200         # 200 miles - covers NYC to Philadelphia\n",
    "\n",
    "# Fare thresholds (dollars)\n",
    "FARE_MIN = 0               # Negative fares are errors (but $0 might be ok)\n",
    "FARE_MAX = 500             # $500 extreme outliers\n",
    "\n",
    "print(\"Validation Thresholds\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Duration: {DURATION_MIN}s - {DURATION_MAX}s (max: {DURATION_MAX/3600:.1f} hrs)\")\n",
    "print(f\"Distance: {DISTANCE_MIN} - {DISTANCE_MAX} miles\")\n",
    "print(f\"Fare: ${FARE_MIN} - ${FARE_MAX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Threshold Selection Notes**\n",
    "\n",
    "The thresholds were selected in order to balance catching data errors without being overly aggressive:\n",
    "\n",
    "- **Duration (60s - 12hr):** 60s minimum filters GPS timing errors while keeping legitimate short trips. 12-hour maximum accommodates long-distance rides (NYC to Philadelphia) based on sampling trips in the 8-12 hour range.\n",
    "\n",
    "- **Distance (0.1 - 200mi):** 0.1 mile minimum filters GPS noise while preserving short local trips. 200-mile maximum covers NYC to Philadelphia service area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Flag Dataset (~20 min)\n",
    "Add validation flags for all quality rules. Each field gets granular flags \n",
    "for specific issues (null, negative, out-of-range, etc.) plus a master \n",
    "validity flag for quick filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating flagged dataset...\n",
      "This will take approximately 20 minutes for 684M records\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81627e433a9047dd844b7b672084f4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagged dataset created: 684,376,551 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating flagged dataset...\")\n",
    "print(\"This will take approximately 20 minutes for 684M records\\n\")\n",
    "\n",
    "# Create flagged dataset with comprehensive validation\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT \n",
    "            *,\n",
    "            \n",
    "            -- ============================================\n",
    "            -- DURATION FLAGS (using trip_time in seconds)\n",
    "            -- ============================================\n",
    "            (trip_time IS NULL) AS flag_duration_null,\n",
    "            (trip_time <= 0) AS flag_duration_zero_negative,\n",
    "            (trip_time < {DURATION_MIN}) AS flag_duration_too_short,\n",
    "            (trip_time > {DURATION_MAX}) AS flag_duration_exceeds_max,\n",
    "            (trip_time > {DURATION_EXTREME}) AS flag_duration_extreme,\n",
    "            \n",
    "            -- ============================================\n",
    "            -- DISTANCE FLAGS (using trip_miles)\n",
    "            -- ============================================\n",
    "            (trip_miles IS NULL) AS flag_distance_null,\n",
    "            (trip_miles < 0) AS flag_distance_negative,\n",
    "            (trip_miles < {DISTANCE_MIN}) AS flag_distance_too_short,\n",
    "            (trip_miles > {DISTANCE_MAX}) AS flag_distance_exceeds_max,\n",
    "            \n",
    "            -- ============================================\n",
    "            -- FARE FLAGS (using base_passenger_fare)\n",
    "            -- ============================================\n",
    "            (base_passenger_fare IS NULL) AS flag_fare_null,\n",
    "            (base_passenger_fare < {FARE_MIN}) AS flag_fare_negative,\n",
    "            (base_passenger_fare = 0) AS flag_fare_zero,\n",
    "            (base_passenger_fare > {FARE_MAX}) AS flag_fare_extreme_high,\n",
    "            \n",
    "            -- ============================================\n",
    "            -- MASTER VALIDITY FLAG\n",
    "            -- Record is valid if ALL critical checks pass\n",
    "            -- ============================================\n",
    "            (\n",
    "                trip_time IS NOT NULL AND\n",
    "                trip_time >= {DURATION_MIN} AND \n",
    "                trip_time <= {DURATION_MAX} AND\n",
    "                trip_miles IS NOT NULL AND\n",
    "                trip_miles >= {DISTANCE_MIN} AND\n",
    "                trip_miles <= {DISTANCE_MAX} AND\n",
    "                base_passenger_fare IS NOT NULL AND\n",
    "                base_passenger_fare >= {FARE_MIN} AND\n",
    "                base_passenger_fare <= {FARE_MAX}\n",
    "            ) AS is_valid\n",
    "            \n",
    "        FROM '{INPUT_FILE}'\n",
    "    ) TO '{FLAGGED_FILE}' (FORMAT PARQUET)\n",
    "\"\"\")\n",
    "\n",
    "# Get record count to confirm success\n",
    "flagged_count = con.execute(f\"SELECT COUNT(*) FROM '{FLAGGED_FILE}'\").fetchone()[0]\n",
    "print(f\"Flagged dataset created: {flagged_count:,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Spot Check Flagged Records\n",
    "Visual verification of flag calculations. Review sample records to confirm flags are applied correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4 Validation Counts \n",
    "Count invalid records from flagged dataset. Each field has its own subsection matching Section 3 structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fa2ecf819c4f3d97c68decbe438cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field Validation Summary\n",
      "================================================================================\n",
      "\n",
      "DURATION (trip_time field)\n",
      "  Total records:               684,376,551\n",
      "  Valid records:               684,316,266\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "  Null values:                           0 ( 0.000%)\n",
      "  Zero/negative:                        65 ( 0.000%)\n",
      "  Too short (<60s):                 60,163 ( 0.009%)\n",
      "  Exceeds 12hr:                         57 ( 0.000%)\n",
      "  Extreme (>7d):                          0 ( 0.000%)\n",
      "\n",
      "DISTANCE (trip_miles field)\n",
      "  Total records:               684,376,551\n",
      "  Valid records:               684,120,480\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "  Null values:                           0 ( 0.000%)\n",
      "  Negative:                              0 ( 0.000%)\n",
      "  Too short (<0.1mi):              251,660 ( 0.037%)\n",
      "  Exceeds 200mi:                    4,411 ( 0.001%)\n",
      "\n",
      "FARE (base_passenger_fare field)\n",
      "  Total records:               684,376,551\n",
      "  Valid records:               684,042,660\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "  Null values:                           0 ( 0.000%)\n",
      "  Negative:                        320,373 ( 0.047%)\n",
      "  Zero fare:                       109,855 ( 0.016%) [Note: might be legit]\n",
      "  Extreme high (>$500):              13,518 ( 0.002%)\n",
      "\n",
      "OVERALL VALIDITY\n",
      "  Valid (all checks):          683,780,462 ( 99.91%)\n",
      "  Invalid (any check):             596,089 (  0.09%)\n"
     ]
    }
   ],
   "source": [
    "# Count all validation flags in one query\n",
    "validation_stats = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total,\n",
    "        \n",
    "        -- Duration issues\n",
    "        SUM(CAST(flag_duration_null AS INTEGER)) as dur_null,\n",
    "        SUM(CAST(flag_duration_zero_negative AS INTEGER)) as dur_zero_neg,\n",
    "        SUM(CAST(flag_duration_too_short AS INTEGER)) as dur_too_short,\n",
    "        SUM(CAST(flag_duration_exceeds_max AS INTEGER)) as dur_exceeds_max,\n",
    "        SUM(CAST(flag_duration_extreme AS INTEGER)) as dur_extreme,\n",
    "        \n",
    "        -- Distance issues\n",
    "        SUM(CAST(flag_distance_null AS INTEGER)) as dist_null,\n",
    "        SUM(CAST(flag_distance_negative AS INTEGER)) as dist_negative,\n",
    "        SUM(CAST(flag_distance_too_short AS INTEGER)) as dist_too_short,\n",
    "        SUM(CAST(flag_distance_exceeds_max AS INTEGER)) as dist_exceeds_max,\n",
    "        \n",
    "        -- Fare issues\n",
    "        SUM(CAST(flag_fare_null AS INTEGER)) as fare_null,\n",
    "        SUM(CAST(flag_fare_negative AS INTEGER)) as fare_negative,\n",
    "        SUM(CAST(flag_fare_zero AS INTEGER)) as fare_zero,\n",
    "        SUM(CAST(flag_fare_extreme_high AS INTEGER)) as fare_extreme_high,\n",
    "        \n",
    "        -- Overall validity\n",
    "        SUM(CAST(is_valid AS INTEGER)) as valid,\n",
    "        SUM(CAST(NOT is_valid AS INTEGER)) as invalid\n",
    "        \n",
    "    FROM '{FLAGGED_FILE}'\n",
    "\"\"\").fetchone()\n",
    "\n",
    "# Unpack results\n",
    "(total, \n",
    " dur_null, dur_zero_neg, dur_too_short, dur_exceeds_max, dur_extreme,\n",
    " dist_null, dist_negative, dist_too_short, dist_exceeds_max,\n",
    " fare_null, fare_negative, fare_zero, fare_extreme_high,\n",
    " valid, invalid) = validation_stats\n",
    "\n",
    "# Display comprehensive validation report\n",
    "print(\"Field Validation Summary\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Duration section\n",
    "print(\"DURATION (trip_time field)\")\n",
    "print(f\"  Total records:           {total:>15,}\")\n",
    "print(f\"  Valid records:           {total - (dur_null + dur_zero_neg + dur_too_short + dur_exceeds_max):>15,}\")\n",
    "print(\"  \" + \"─\" * 76)\n",
    "print(f\"  Null values:             {dur_null:>15,} ({dur_null/total*100:>6.3f}%)\")\n",
    "print(f\"  Zero/negative:           {dur_zero_neg:>15,} ({dur_zero_neg/total*100:>6.3f}%)\")\n",
    "print(f\"  Too short (<{DURATION_MIN}s):        {dur_too_short:>15,} ({dur_too_short/total*100:>6.3f}%)\")\n",
    "print(f\"  Exceeds {DURATION_MAX/3600:.0f}hr:            {dur_exceeds_max:>15,} ({dur_exceeds_max/total*100:>6.3f}%)\")\n",
    "print(f\"  Extreme (>{DURATION_EXTREME/86400:.0f}d):            {dur_extreme:>15,} ({dur_extreme/total*100:>6.3f}%)\")\n",
    "print()\n",
    "\n",
    "# Distance section\n",
    "print(\"DISTANCE (trip_miles field)\")\n",
    "print(f\"  Total records:           {total:>15,}\")\n",
    "print(f\"  Valid records:           {total - (dist_null + dist_negative + dist_too_short + dist_exceeds_max):>15,}\")\n",
    "print(\"  \" + \"─\" * 76)\n",
    "print(f\"  Null values:             {dist_null:>15,} ({dist_null/total*100:>6.3f}%)\")\n",
    "print(f\"  Negative:                {dist_negative:>15,} ({dist_negative/total*100:>6.3f}%)\")\n",
    "print(f\"  Too short (<{DISTANCE_MIN}mi):      {dist_too_short:>15,} ({dist_too_short/total*100:>6.3f}%)\")\n",
    "print(f\"  Exceeds {DISTANCE_MAX}mi:          {dist_exceeds_max:>15,} ({dist_exceeds_max/total*100:>6.3f}%)\")\n",
    "print()\n",
    "\n",
    "# Fare section\n",
    "print(\"FARE (base_passenger_fare field)\")\n",
    "print(f\"  Total records:           {total:>15,}\")\n",
    "print(f\"  Valid records:           {total - (fare_null + fare_negative + fare_extreme_high):>15,}\")\n",
    "print(\"  \" + \"─\" * 76)\n",
    "print(f\"  Null values:             {fare_null:>15,} ({fare_null/total*100:>6.3f}%)\")\n",
    "print(f\"  Negative:                {fare_negative:>15,} ({fare_negative/total*100:>6.3f}%)\")\n",
    "print(f\"  Zero fare:               {fare_zero:>15,} ({fare_zero/total*100:>6.3f}%) [Note: might be legit]\")\n",
    "print(f\"  Extreme high (>${FARE_MAX}):     {fare_extreme_high:>15,} ({fare_extreme_high/total*100:>6.3f}%)\")\n",
    "print()\n",
    "\n",
    "# Overall section\n",
    "print(\"OVERALL VALIDITY\")\n",
    "print(f\"  Valid (all checks):      {valid:>15,} ({valid/total*100:>6.2f}%)\")\n",
    "print(f\"  Invalid (any check):     {invalid:>15,} ({invalid/total*100:>6.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5 Create EDA Dataset (~25 min)\n",
    "Save data to a parquet file with only valid records, removing flag columns. This creates a clean dataset for exploratory analysis without validation overhead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930fc22c079d47b3b5dd2478e2d52808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA dataset: fhvhv_valid_data_for_eda.parquet (683,780,462 records)\n"
     ]
    }
   ],
   "source": [
    "# Create EDA dataset with only valid records, excluding flag columns\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT * EXCLUDE (\n",
    "            flag_duration_null, \n",
    "            flag_duration_zero_negative, \n",
    "            flag_duration_too_short, \n",
    "            flag_duration_exceeds_max, \n",
    "            flag_duration_extreme,\n",
    "            flag_distance_null, \n",
    "            flag_distance_negative, \n",
    "            flag_distance_too_short, \n",
    "            flag_distance_exceeds_max,\n",
    "            flag_fare_null, \n",
    "            flag_fare_negative, \n",
    "            flag_fare_zero, \n",
    "            flag_fare_extreme_high,\n",
    "            is_valid\n",
    "        )\n",
    "        FROM '{FLAGGED_FILE}'\n",
    "        WHERE is_valid = true\n",
    "    ) TO '{EDA_FILE}' (FORMAT PARQUET)\n",
    "\"\"\")\n",
    "print(f\"EDA dataset: {EDA_FILE.name} ({valid:,} records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6 Generate Validation Report\n",
    "Create detailed CSV report of all validation checks for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation report saved: ..\\data\\quality_reports\\validation_report.csv\n"
     ]
    }
   ],
   "source": [
    "# Build comprehensive report DataFrame\n",
    "report_data = []\n",
    "\n",
    "# Duration rows\n",
    "report_data.append({\n",
    "    'field': 'duration',\n",
    "    'rule': 'null',\n",
    "    'invalid_count': dur_null,\n",
    "    'invalid_pct': dur_null/total*100,\n",
    "    'threshold': 'IS NULL'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'duration',\n",
    "    'rule': 'zero_negative',\n",
    "    'invalid_count': dur_zero_neg,\n",
    "    'invalid_pct': dur_zero_neg/total*100,\n",
    "    'threshold': '<= 0'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'duration',\n",
    "    'rule': 'too_short',\n",
    "    'invalid_count': dur_too_short,\n",
    "    'invalid_pct': dur_too_short/total*100,\n",
    "    'threshold': f'< {DURATION_MIN}s'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'duration',\n",
    "    'rule': 'exceeds_max',\n",
    "    'invalid_count': dur_exceeds_max,\n",
    "    'invalid_pct': dur_exceeds_max/total*100,\n",
    "    'threshold': f'> {DURATION_MAX}s'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'duration',\n",
    "    'rule': 'extreme',\n",
    "    'invalid_count': dur_extreme,\n",
    "    'invalid_pct': dur_extreme/total*100,\n",
    "    'threshold': f'> {DURATION_EXTREME}s'\n",
    "})\n",
    "\n",
    "# Distance rows\n",
    "report_data.append({\n",
    "    'field': 'distance',\n",
    "    'rule': 'null',\n",
    "    'invalid_count': dist_null,\n",
    "    'invalid_pct': dist_null/total*100,\n",
    "    'threshold': 'IS NULL'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'distance',\n",
    "    'rule': 'negative',\n",
    "    'invalid_count': dist_negative,\n",
    "    'invalid_pct': dist_negative/total*100,\n",
    "    'threshold': '< 0'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'distance',\n",
    "    'rule': 'too_short',\n",
    "    'invalid_count': dist_too_short,\n",
    "    'invalid_pct': dist_too_short/total*100,\n",
    "    'threshold': f'< {DISTANCE_MIN}mi'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'distance',\n",
    "    'rule': 'exceeds_max',\n",
    "    'invalid_count': dist_exceeds_max,\n",
    "    'invalid_pct': dist_exceeds_max/total*100,\n",
    "    'threshold': f'> {DISTANCE_MAX}mi'\n",
    "})\n",
    "\n",
    "# Fare rows\n",
    "report_data.append({\n",
    "    'field': 'fare',\n",
    "    'rule': 'null',\n",
    "    'invalid_count': fare_null,\n",
    "    'invalid_pct': fare_null/total*100,\n",
    "    'threshold': 'IS NULL'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'fare',\n",
    "    'rule': 'negative',\n",
    "    'invalid_count': fare_negative,\n",
    "    'invalid_pct': fare_negative/total*100,\n",
    "    'threshold': '< 0'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'fare',\n",
    "    'rule': 'zero',\n",
    "    'invalid_count': fare_zero,\n",
    "    'invalid_pct': fare_zero/total*100,\n",
    "    'threshold': '= 0'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'fare',\n",
    "    'rule': 'extreme_high',\n",
    "    'invalid_count': fare_extreme_high,\n",
    "    'invalid_pct': fare_extreme_high/total*100,\n",
    "    'threshold': f'> ${FARE_MAX}'\n",
    "})\n",
    "\n",
    "# Overall totals\n",
    "report_data.append({\n",
    "    'field': 'OVERALL',\n",
    "    'rule': 'VALID',\n",
    "    'invalid_count': valid,\n",
    "    'invalid_pct': valid/total*100,\n",
    "    'threshold': 'All checks pass'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'OVERALL',\n",
    "    'rule': 'INVALID',\n",
    "    'invalid_count': invalid,\n",
    "    'invalid_pct': invalid/total*100,\n",
    "    'threshold': 'Any check fails'\n",
    "})\n",
    "\n",
    "# Create DataFrame and add metadata\n",
    "report = pd.DataFrame(report_data)\n",
    "report['total_records'] = total\n",
    "report['processing_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Reorder columns for better readability\n",
    "report = report[['field', 'rule', 'threshold', 'invalid_count', 'invalid_pct', 'total_records', 'processing_date']]\n",
    "\n",
    "# Save report\n",
    "report.to_csv(REPORTS_DIR / \"validation_report.csv\", index=False)\n",
    "print(f\"Validation report saved: {REPORTS_DIR / 'validation_report.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Cleanup\n",
    "Close database connections and release resources to complete the validation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete\n"
     ]
    }
   ],
   "source": [
    "# Close DuckDB connection\n",
    "con.close()\n",
    "print(\"Pipeline complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "**Execution Results**\n",
    "\n",
    "This validation pipeline processed 684,376,551 NYC rideshare trip records from 2022-2024, implementing 13 quality checks across duration, distance, and fare fields. The analysis showed excellent data quality with 683,780,462 valid records (99.91%) and only 596,089 invalid records (0.09%). Processing completed in approximately 70 minutes.\n",
    "\n",
    "**Dataset Overview**\n",
    "\n",
    "The validation pipeline produced three output datasets:\n",
    "- 684,376,551 total records processed\n",
    "- 683,780,462 valid records (99.91% pass rate)\n",
    "- 596,089 invalid records (0.09% with quality issues)\n",
    "- 13 granular quality flags implemented\n",
    "- 3 output files created\n",
    "\n",
    "**Data Quality Observations**\n",
    "\n",
    "The validation results reveal an exceptionally clean dataset. All three validated fields (duration, distance, fare) show pass rates above 99.95%.\n",
    "\n",
    "Field-specific findings:\n",
    "- **Duration:** 99.99% pass rate - only 60,163 trips under 60 seconds and 57 exceeding 12 hours\n",
    "- **Distance:** 99.96% pass rate - only 4,411 trips over 200 miles\n",
    "- **Fare:** 99.95% pass rate - 320K negative fares may need investigation, 110K zero-fare trips retained\n",
    "\n",
    "Three columns show significant nulls but don't impact demand forecasting:\n",
    "- `originating_base_num` (27% null)\n",
    "- `on_scene_datetime` (27% null)\n",
    "- `airport_fee` (18% null)\n",
    "\n",
    "**Technical Decisions**\n",
    "\n",
    "Boolean validation flags were implemented to preserve all records while marking quality issues. This strategy retains all data in the original dataset while creating a clean dataset for analysis. Zero-fare trips were intentionally retained as they may represent legitimate demand (promotional rides, driver incentives) despite no payment. Negative fares were excluded as they likely represent refunds or cancellations rather than completed trips.\n",
    "\n",
    "\n",
    "**Output Files**\n",
    "- `fhvhv_all_data_flagged.parquet` - All 684M records with 14 validation columns for quality monitoring\n",
    "- `fhvhv_valid_data_for_eda.parquet` - 683.8M valid records (24 original columns) for analysis\n",
    "- `validation_report.csv` - Detailed metrics for all 13 validation rules\n",
    "\n",
    "**Next Steps**\n",
    "\n",
    "Proceed to **02_exploratory_analysis.ipynb** to:\n",
    "- Aggregate trips by borough and time period\n",
    "- Analyze demand patterns (daily, weekly, seasonal)\n",
    "- Engineer features for forecasting models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
