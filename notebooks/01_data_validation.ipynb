{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FHVHV Data Validation - Stage 1\n",
    "\n",
    "**Pipeline Position:** Stage 1 of 4\n",
    "- Stage 0: Data Download \n",
    "- Stage 1: Data Validation ← THIS SCRIPT\n",
    "- Stage 2: Exploratory Analysis\n",
    "- Stage 3: Modeling\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This notebook validates NYC TLC FHVHV trip data (Uber/Lyft) for 2022-2024. It implements a comprehensive validation framework with granular quality checks across trip duration, distance, and fare fields. The pipeline uses DuckDB for memory-efficient processing of 684M records and creates detailed quality flags for monitoring and debugging.\n",
    "\n",
    "**Validation Scope: 3 of 24 Columns**\n",
    "\n",
    "This validation focuses on **quantitative outcome fields** critical for demand forecasting:\n",
    "- **trip_time (Duration):** Temporal patterns affecting vehicle turnaround (60s - 12hr bounds)\n",
    "- **trip_miles (Distance):** Spatial distribution of demand (0.1 - 200mi bounds)\n",
    "- **base_passenger_fare (Fare):** Economic demand indicators ($0 - $500 bounds)\n",
    "\n",
    "**Why only these 3?** The remaining 21 columns are either:\n",
    "- *Identifiers* (categorical, no invalid states)\n",
    "- *Timestamps* (validated indirectly via duration)\n",
    "- *Location IDs* (validated during borough aggregation)\n",
    "- *Fee components* (not used in demand forecasting)\n",
    "- *Boolean flags* (self-validating Y/N states)\n",
    "\n",
    "This is **use-case driven validation**: validate what impacts your analysis objective.\n",
    "\n",
    "**Validation Strategy**\n",
    "- **13 granular flags** for specific quality issues (null, negative, out-of-bounds, extreme)\n",
    "- **Master validity indicator** for quick filtering (99.95% expected pass rate)\n",
    "- **Configurable thresholds** enable easy adaptation to future projects (water quality forecasting)\n",
    "- **Flag-based approach** preserves all records for audit trails and debugging\n",
    "\n",
    "**How Reusability Works**\n",
    "- Threshold variables defined at top (change 3 lines for new project)\n",
    "- Domain-agnostic validation pattern (null→negative→bounds→extreme)\n",
    "- Modular flag architecture (add/remove fields independently)\n",
    "- Portable DuckDB queries (same SQL structure for different datasets)\n",
    "\n",
    "**Output Files**\n",
    "- `data/validated/fhvhv_all_data_flagged.parquet` - All records with 13 validation flags\n",
    "- `data/validated/fhvhv_valid_data_for_eda.parquet` - Valid records only (99.95%)\n",
    "- `data/quality_reports/validation_report.csv` - Detailed validation metrics\n",
    "\n",
    "**Runtime Note:** \n",
    "- Section 3.5 (Flagging): ~20 minutes\n",
    "- Section 4.5 (EDA dataset): ~25 minutes\n",
    "- Total: ~70 minutes\n",
    "\n",
    "**Next Step:** Run `02_exploratory_analysis.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why This Approach Matters**\n",
    "This validation framework prioritizes auditability, debuggability, and reusability—key \n",
    "principles for production data pipelines. The granular flagging strategy enables both \n",
    "high-level quality monitoring and detailed root cause analysis while preserving data \n",
    "flexibility for different use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ..\\data\\final\\combined_fhvhv_tripdata.parquet\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "# Define file paths for input and output use relative paths so notebook works from any machine with same folder structure\n",
    "INPUT_FILE = Path(\"../data/final/combined_fhvhv_tripdata.parquet\")\n",
    "OUTPUT_DIR = Path(\"../data/validated\")\n",
    "REPORTS_DIR = Path(\"../data/quality_reports\")\n",
    "\n",
    "# Create directories if  don't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#  Define output file names\n",
    "FLAGGED_FILE = OUTPUT_DIR / \"fhvhv_all_data_flagged.parquet\"\n",
    "EDA_FILE = OUTPUT_DIR / \"fhvhv_valid_data_for_eda.parquet\"\n",
    "\n",
    "# Initialize DuckDB connection and set progress bar options\n",
    "con = duckdb.connect()\n",
    "con.execute(\"SET enable_progress_bar = true\")\n",
    "con.execute(\"SET progress_bar_time = 2000\")\n",
    "\n",
    "print(f\"Input: {INPUT_FILE}\")\n",
    "print(f\"File exists: {INPUT_FILE.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Exploration\n",
    "Review schema overview and identify missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Column Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 684,376,551\n",
      "\n",
      "Dataset has 24 columns:\n",
      "\n",
      "         column_name column_type null  key default extra\n",
      "   hvfhs_license_num     VARCHAR  YES None    None  None\n",
      "dispatching_base_num     VARCHAR  YES None    None  None\n",
      "originating_base_num     VARCHAR  YES None    None  None\n",
      "    request_datetime   TIMESTAMP  YES None    None  None\n",
      "   on_scene_datetime   TIMESTAMP  YES None    None  None\n",
      "     pickup_datetime   TIMESTAMP  YES None    None  None\n",
      "    dropoff_datetime   TIMESTAMP  YES None    None  None\n",
      "        PULocationID      BIGINT  YES None    None  None\n",
      "        DOLocationID      BIGINT  YES None    None  None\n",
      "          trip_miles      DOUBLE  YES None    None  None\n",
      "           trip_time      BIGINT  YES None    None  None\n",
      " base_passenger_fare      DOUBLE  YES None    None  None\n",
      "               tolls      DOUBLE  YES None    None  None\n",
      "                 bcf      DOUBLE  YES None    None  None\n",
      "           sales_tax      DOUBLE  YES None    None  None\n",
      "congestion_surcharge      DOUBLE  YES None    None  None\n",
      "         airport_fee      DOUBLE  YES None    None  None\n",
      "                tips      DOUBLE  YES None    None  None\n",
      "          driver_pay      DOUBLE  YES None    None  None\n",
      " shared_request_flag     VARCHAR  YES None    None  None\n",
      "   shared_match_flag     VARCHAR  YES None    None  None\n",
      "  access_a_ride_flag     VARCHAR  YES None    None  None\n",
      "    wav_request_flag     VARCHAR  YES None    None  None\n",
      "      wav_match_flag     VARCHAR  YES None    None  None\n"
     ]
    }
   ],
   "source": [
    "# Get total record count\n",
    "total_records = con.execute(f\"SELECT COUNT(*) FROM '{INPUT_FILE}'\").fetchone()[0]\n",
    "print(f\"Total records: {total_records:,}\\n\")\n",
    "\n",
    "# Review column names and data types using DESCRIBE\n",
    "schema = con.execute(f\"\"\"\n",
    "    DESCRIBE SELECT * FROM '{INPUT_FILE}'\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Dataset has {len(schema)} columns:\\n\")\n",
    "print(schema.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Date Range Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2022-01-01 00:00:00 to 2024-12-31 23:59:59\n"
     ]
    }
   ],
   "source": [
    "# Check date range of pickup_datetime to verify coverage period\n",
    "date_range = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        MIN(pickup_datetime) as earliest,\n",
    "        MAX(pickup_datetime) as latest\n",
    "    FROM '{INPUT_FILE}'\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Date range: {date_range['earliest'].iloc[0]} to {date_range['latest'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 Missing Values Check\n",
    "Identify null values by column. High-null columns will be excluded during aggregation in EDA, not removed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e49beec2e440f39ce58c761aa8ccdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column                    Null Count   Null Percentage\n",
      "--------------------------------------------------\n",
      "hvfhs_license_num         0                       0.00%\n",
      "dispatching_base_num      0                       0.00%\n",
      "originating_base_num      183954837              26.88%\n",
      "request_datetime          0                       0.00%\n",
      "on_scene_datetime         183891654              26.87%\n",
      "pickup_datetime           0                       0.00%\n",
      "dropoff_datetime          0                       0.00%\n",
      "PULocationID              0                       0.00%\n",
      "DOLocationID              0                       0.00%\n",
      "trip_miles                0                       0.00%\n",
      "trip_time                 0                       0.00%\n",
      "base_passenger_fare       0                       0.00%\n",
      "tolls                     0                       0.00%\n",
      "bcf                       0                       0.00%\n",
      "sales_tax                 0                       0.00%\n",
      "congestion_surcharge      0                       0.00%\n",
      "airport_fee               0                       0.00%\n",
      "tips                      0                       0.00%\n",
      "driver_pay                0                       0.00%\n",
      "shared_request_flag       0                       0.00%\n",
      "shared_match_flag         0                       0.00%\n",
      "access_a_ride_flag        0                       0.00%\n",
      "wav_request_flag          0                       0.00%\n",
      "wav_match_flag            0                       0.00%\n"
     ]
    }
   ],
   "source": [
    "# Get list of column names from schema\n",
    "columns = schema['column_name'].tolist()\n",
    "\n",
    "# Build SQL to count NULLs per column using CASE WHEN use single scan counts nulls for all columns once\n",
    "null_count_sql = f\"\"\"\n",
    "    SELECT \n",
    "        {', '.join([f\"SUM(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) AS {col}_null_count\" for col in columns])}\n",
    "    FROM '{INPUT_FILE}'\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and get results as tuple\n",
    "null_counts = con.execute(null_count_sql).fetchone()\n",
    "\n",
    "# Calculate null percentages f\n",
    "null_pct = [(count / total_records) * 100 for count in null_counts]\n",
    "\n",
    "#Display results \n",
    "print(f\"{'Column':<25} {'Null Count':<12} {'Null Percentage'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for col, count, pct in zip(columns, null_counts, null_pct):\n",
    "    print(f\"{col:<25} {count:<12} {pct:>15.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**High-null columns (>10%):**\n",
    "- `originating_base_num` (27%) - Lyft doesn't report this field\n",
    "- `on_scene_datetime` (27%) - Optional tracking field\n",
    "- `airport_fee` (18%) - Only applies to airport trips\n",
    "\n",
    "**Impact on forecasting:**\n",
    "These columns are most likely not needed for trip count forecasting and will be excluded during aggregation in EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Validation\n",
    "This section defines validation thresholds and flags invalid records in a single pass:\n",
    "- **Duration:** Null checks, bounds (60s - 12hr), extreme detection (>7 days)\n",
    "- **Distance:** Null checks, negative values, bounds (0.1 - 200mi)\n",
    "- **Fare:** Null checks, negative values, bounds ($0 - $500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Thresholds\n",
      "==================================================\n",
      "Duration: 60s - 43200s (max: 12.0 hrs)\n",
      "Distance: 0.1 - 200 miles\n",
      "Fare: $0 - $500\n"
     ]
    }
   ],
   "source": [
    "# Duration thresholds (trip_time is in seconds)\n",
    "DURATION_MIN = 60          # 1 minute - shorter likely GPS/timing errors\n",
    "DURATION_MAX = 43200       # 12 hours - reasonable max for rideshare\n",
    "DURATION_EXTREME = 604800  # 7 days - clear data corruption\n",
    "\n",
    "# Distance thresholds (trip_miles is in miles)\n",
    "DISTANCE_MIN = 0.1         # 0.1 miles - filters GPS noise\n",
    "DISTANCE_MAX = 200         # 200 miles - covers NYC to Philadelphia\n",
    "\n",
    "# Fare thresholds (base_passenger_fare is in dollars)\n",
    "FARE_MIN = 0               # Negative fares are errors (but $0 might be legit)\n",
    "FARE_MAX = 500             # $500 - catches extreme outliers\n",
    "\n",
    "print(\"Validation Thresholds\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Duration: {DURATION_MIN}s - {DURATION_MAX}s (max: {DURATION_MAX/3600:.1f} hrs)\")\n",
    "print(f\"Distance: {DISTANCE_MIN} - {DISTANCE_MAX} miles\")\n",
    "print(f\"Fare: ${FARE_MIN} - ${FARE_MAX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Threshold Selection Notes**\n",
    "\n",
    "The thresholds were selected in order to balance catching data errors without being overly aggressive:\n",
    "\n",
    "- **Duration (60s - 12hr):** 60s minimum filters GPS timing errors while keeping legitimate short trips. 12-hour maximum accommodates long-distance rides (NYC to Philadelphia) based on sampling trips in the 8-12 hour range.\n",
    "\n",
    "- **Distance (0.1 - 200mi):** 0.1 mile minimum filters GPS noise while preserving short local trips. 200-mile maximum covers NYC to Philadelphia service area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Flag Dataset (~20 min)\n",
    "Add validation flags for all quality rules. Each field gets granular flags \n",
    "for specific issues (null, negative, out-of-range, etc.) plus a master \n",
    "validity flag for quick filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating flagged dataset...\n",
      "This will take approximately 20 minutes for 684M records\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e1aa28c35942ff967975d35afdd7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagged dataset created: 684,376,551 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating flagged dataset...\")\n",
    "print(\"This will take approximately 20 minutes for 684M records\\n\")\n",
    "\n",
    "# Create flagged dataset with comprehensive validation\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT \n",
    "            *,\n",
    "            \n",
    "            -- ============================================\n",
    "            -- DURATION FLAGS (using trip_time in seconds)\n",
    "            -- ============================================\n",
    "            (trip_time IS NULL) AS flag_duration_null,\n",
    "            (trip_time <= 0) AS flag_duration_zero_negative,\n",
    "            (trip_time < {DURATION_MIN}) AS flag_duration_too_short,\n",
    "            (trip_time > {DURATION_MAX}) AS flag_duration_exceeds_max,\n",
    "            (trip_time > {DURATION_EXTREME}) AS flag_duration_extreme,\n",
    "            \n",
    "            -- ============================================\n",
    "            -- DISTANCE FLAGS (using trip_miles)\n",
    "            -- ============================================\n",
    "            (trip_miles IS NULL) AS flag_distance_null,\n",
    "            (trip_miles < 0) AS flag_distance_negative,\n",
    "            (trip_miles < {DISTANCE_MIN}) AS flag_distance_too_short,\n",
    "            (trip_miles > {DISTANCE_MAX}) AS flag_distance_exceeds_max,\n",
    "            \n",
    "            -- ============================================\n",
    "            -- FARE FLAGS (using base_passenger_fare)\n",
    "            -- ============================================\n",
    "            (base_passenger_fare IS NULL) AS flag_fare_null,\n",
    "            (base_passenger_fare < {FARE_MIN}) AS flag_fare_negative,\n",
    "            (base_passenger_fare = 0) AS flag_fare_zero,\n",
    "            (base_passenger_fare > {FARE_MAX}) AS flag_fare_extreme_high,\n",
    "            \n",
    "            -- ============================================\n",
    "            -- MASTER VALIDITY FLAG\n",
    "            -- Record is valid if ALL critical checks pass\n",
    "            -- ============================================\n",
    "            (\n",
    "                trip_time IS NOT NULL AND\n",
    "                trip_time >= {DURATION_MIN} AND \n",
    "                trip_time <= {DURATION_MAX} AND\n",
    "                trip_miles IS NOT NULL AND\n",
    "                trip_miles >= {DISTANCE_MIN} AND\n",
    "                trip_miles <= {DISTANCE_MAX} AND\n",
    "                base_passenger_fare IS NOT NULL AND\n",
    "                base_passenger_fare >= {FARE_MIN} AND\n",
    "                base_passenger_fare <= {FARE_MAX}\n",
    "            ) AS is_valid\n",
    "            \n",
    "        FROM '{INPUT_FILE}'\n",
    "    ) TO '{FLAGGED_FILE}' (FORMAT PARQUET)\n",
    "\"\"\")\n",
    "\n",
    "# Get record count to confirm success\n",
    "flagged_count = con.execute(f\"SELECT COUNT(*) FROM '{FLAGGED_FILE}'\").fetchone()[0]\n",
    "print(f\"Flagged dataset created: {flagged_count:,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Spot Check Flagged Records\n",
    "Visual verification of flag calculations. Review sample records to confirm flags are applied correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4 Validation Counts \n",
    "Count invalid records from flagged dataset. Each field has its own subsection matching Section 3 structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937875537eb5408c8b06bc73929459af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field Validation Summary\n",
      "================================================================================\n",
      "\n",
      "DURATION (trip_time field)\n",
      "  Total records:               684,376,551\n",
      "  Valid records:               684,316,266\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "  Null values:                           0 ( 0.000%)\n",
      "  Zero/negative:                        65 ( 0.000%)\n",
      "  Too short (<60s):                 60,163 ( 0.009%)\n",
      "  Exceeds 12hr:                         57 ( 0.000%)\n",
      "  Extreme (>7d):                          0 ( 0.000%)\n",
      "\n",
      "DISTANCE (trip_miles field)\n",
      "  Total records:               684,376,551\n",
      "  Valid records:               684,120,480\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "  Null values:                           0 ( 0.000%)\n",
      "  Negative:                              0 ( 0.000%)\n",
      "  Too short (<0.1mi):              251,660 ( 0.037%)\n",
      "  Exceeds 200mi:                    4,411 ( 0.001%)\n",
      "\n",
      "FARE (base_passenger_fare field)\n",
      "  Total records:               684,376,551\n",
      "  Valid records:               684,042,660\n",
      "  ────────────────────────────────────────────────────────────────────────────\n",
      "  Null values:                           0 ( 0.000%)\n",
      "  Negative:                        320,373 ( 0.047%)\n",
      "  Zero fare:                       109,855 ( 0.016%) [Note: might be legit]\n",
      "  Extreme high (>$500):              13,518 ( 0.002%)\n",
      "\n",
      "OVERALL VALIDITY\n",
      "  Valid (all checks):          683,780,462 ( 99.91%)\n",
      "  Invalid (any check):             596,089 (  0.09%)\n"
     ]
    }
   ],
   "source": [
    "# Count all validation flags in one query\n",
    "validation_stats = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total,\n",
    "        \n",
    "        -- Duration issues\n",
    "        SUM(CAST(flag_duration_null AS INTEGER)) as dur_null,\n",
    "        SUM(CAST(flag_duration_zero_negative AS INTEGER)) as dur_zero_neg,\n",
    "        SUM(CAST(flag_duration_too_short AS INTEGER)) as dur_too_short,\n",
    "        SUM(CAST(flag_duration_exceeds_max AS INTEGER)) as dur_exceeds_max,\n",
    "        SUM(CAST(flag_duration_extreme AS INTEGER)) as dur_extreme,\n",
    "        \n",
    "        -- Distance issues\n",
    "        SUM(CAST(flag_distance_null AS INTEGER)) as dist_null,\n",
    "        SUM(CAST(flag_distance_negative AS INTEGER)) as dist_negative,\n",
    "        SUM(CAST(flag_distance_too_short AS INTEGER)) as dist_too_short,\n",
    "        SUM(CAST(flag_distance_exceeds_max AS INTEGER)) as dist_exceeds_max,\n",
    "        \n",
    "        -- Fare issues\n",
    "        SUM(CAST(flag_fare_null AS INTEGER)) as fare_null,\n",
    "        SUM(CAST(flag_fare_negative AS INTEGER)) as fare_negative,\n",
    "        SUM(CAST(flag_fare_zero AS INTEGER)) as fare_zero,\n",
    "        SUM(CAST(flag_fare_extreme_high AS INTEGER)) as fare_extreme_high,\n",
    "        \n",
    "        -- Overall validity\n",
    "        SUM(CAST(is_valid AS INTEGER)) as valid,\n",
    "        SUM(CAST(NOT is_valid AS INTEGER)) as invalid\n",
    "        \n",
    "    FROM '{FLAGGED_FILE}'\n",
    "\"\"\").fetchone()\n",
    "\n",
    "# Unpack results\n",
    "(total, \n",
    " dur_null, dur_zero_neg, dur_too_short, dur_exceeds_max, dur_extreme,\n",
    " dist_null, dist_negative, dist_too_short, dist_exceeds_max,\n",
    " fare_null, fare_negative, fare_zero, fare_extreme_high,\n",
    " valid, invalid) = validation_stats\n",
    "\n",
    "# Display comprehensive validation report\n",
    "print(\"Field Validation Summary\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Duration section\n",
    "print(\"DURATION (trip_time field)\")\n",
    "print(f\"  Total records:           {total:>15,}\")\n",
    "print(f\"  Valid records:           {total - (dur_null + dur_zero_neg + dur_too_short + dur_exceeds_max):>15,}\")\n",
    "print(\"  \" + \"─\" * 76)\n",
    "print(f\"  Null values:             {dur_null:>15,} ({dur_null/total*100:>6.3f}%)\")\n",
    "print(f\"  Zero/negative:           {dur_zero_neg:>15,} ({dur_zero_neg/total*100:>6.3f}%)\")\n",
    "print(f\"  Too short (<{DURATION_MIN}s):        {dur_too_short:>15,} ({dur_too_short/total*100:>6.3f}%)\")\n",
    "print(f\"  Exceeds {DURATION_MAX/3600:.0f}hr:            {dur_exceeds_max:>15,} ({dur_exceeds_max/total*100:>6.3f}%)\")\n",
    "print(f\"  Extreme (>{DURATION_EXTREME/86400:.0f}d):            {dur_extreme:>15,} ({dur_extreme/total*100:>6.3f}%)\")\n",
    "print()\n",
    "\n",
    "# Distance section\n",
    "print(\"DISTANCE (trip_miles field)\")\n",
    "print(f\"  Total records:           {total:>15,}\")\n",
    "print(f\"  Valid records:           {total - (dist_null + dist_negative + dist_too_short + dist_exceeds_max):>15,}\")\n",
    "print(\"  \" + \"─\" * 76)\n",
    "print(f\"  Null values:             {dist_null:>15,} ({dist_null/total*100:>6.3f}%)\")\n",
    "print(f\"  Negative:                {dist_negative:>15,} ({dist_negative/total*100:>6.3f}%)\")\n",
    "print(f\"  Too short (<{DISTANCE_MIN}mi):      {dist_too_short:>15,} ({dist_too_short/total*100:>6.3f}%)\")\n",
    "print(f\"  Exceeds {DISTANCE_MAX}mi:          {dist_exceeds_max:>15,} ({dist_exceeds_max/total*100:>6.3f}%)\")\n",
    "print()\n",
    "\n",
    "# Fare section\n",
    "print(\"FARE (base_passenger_fare field)\")\n",
    "print(f\"  Total records:           {total:>15,}\")\n",
    "print(f\"  Valid records:           {total - (fare_null + fare_negative + fare_extreme_high):>15,}\")\n",
    "print(\"  \" + \"─\" * 76)\n",
    "print(f\"  Null values:             {fare_null:>15,} ({fare_null/total*100:>6.3f}%)\")\n",
    "print(f\"  Negative:                {fare_negative:>15,} ({fare_negative/total*100:>6.3f}%)\")\n",
    "print(f\"  Zero fare:               {fare_zero:>15,} ({fare_zero/total*100:>6.3f}%) [Note: might be legit]\")\n",
    "print(f\"  Extreme high (>${FARE_MAX}):     {fare_extreme_high:>15,} ({fare_extreme_high/total*100:>6.3f}%)\")\n",
    "print()\n",
    "\n",
    "# Overall section\n",
    "print(\"OVERALL VALIDITY\")\n",
    "print(f\"  Valid (all checks):      {valid:>15,} ({valid/total*100:>6.2f}%)\")\n",
    "print(f\"  Invalid (any check):     {invalid:>15,} ({invalid/total*100:>6.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5 Create EDA Dataset (~25 min)\n",
    "Save data to a parquet file with only valid records, removing flag columns. This creates a clean dataset for exploratory analysis without validation overhead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e5138b219541699f6596766e7df6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA dataset: fhvhv_valid_data_for_eda.parquet (683,780,462 records)\n"
     ]
    }
   ],
   "source": [
    "# Create EDA dataset with only valid records, excluding flag columns\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT * EXCLUDE (\n",
    "            flag_duration_null, \n",
    "            flag_duration_zero_negative, \n",
    "            flag_duration_too_short, \n",
    "            flag_duration_exceeds_max, \n",
    "            flag_duration_extreme,\n",
    "            flag_distance_null, \n",
    "            flag_distance_negative, \n",
    "            flag_distance_too_short, \n",
    "            flag_distance_exceeds_max,\n",
    "            flag_fare_null, \n",
    "            flag_fare_negative, \n",
    "            flag_fare_zero, \n",
    "            flag_fare_extreme_high,\n",
    "            is_valid\n",
    "        )\n",
    "        FROM '{FLAGGED_FILE}'\n",
    "        WHERE is_valid = true\n",
    "    ) TO '{EDA_FILE}' (FORMAT PARQUET)\n",
    "\"\"\")\n",
    "print(f\"EDA dataset: {EDA_FILE.name} ({valid:,} records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6 Generate Validation Report\n",
    "Create detailed CSV report of all validation checks for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation report saved: ..\\data\\quality_reports\\validation_report.csv\n"
     ]
    }
   ],
   "source": [
    "# Build comprehensive report DataFrame\n",
    "report_data = []\n",
    "\n",
    "# Duration rows\n",
    "report_data.append({\n",
    "    'field': 'duration',\n",
    "    'rule': 'null',\n",
    "    'invalid_count': dur_null,\n",
    "    'invalid_pct': dur_null/total*100,\n",
    "    'threshold': 'IS NULL'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'duration',\n",
    "    'rule': 'zero_negative',\n",
    "    'invalid_count': dur_zero_neg,\n",
    "    'invalid_pct': dur_zero_neg/total*100,\n",
    "    'threshold': '<= 0'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'duration',\n",
    "    'rule': 'too_short',\n",
    "    'invalid_count': dur_too_short,\n",
    "    'invalid_pct': dur_too_short/total*100,\n",
    "    'threshold': f'< {DURATION_MIN}s'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'duration',\n",
    "    'rule': 'exceeds_max',\n",
    "    'invalid_count': dur_exceeds_max,\n",
    "    'invalid_pct': dur_exceeds_max/total*100,\n",
    "    'threshold': f'> {DURATION_MAX}s'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'duration',\n",
    "    'rule': 'extreme',\n",
    "    'invalid_count': dur_extreme,\n",
    "    'invalid_pct': dur_extreme/total*100,\n",
    "    'threshold': f'> {DURATION_EXTREME}s'\n",
    "})\n",
    "\n",
    "# Distance rows\n",
    "report_data.append({\n",
    "    'field': 'distance',\n",
    "    'rule': 'null',\n",
    "    'invalid_count': dist_null,\n",
    "    'invalid_pct': dist_null/total*100,\n",
    "    'threshold': 'IS NULL'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'distance',\n",
    "    'rule': 'negative',\n",
    "    'invalid_count': dist_negative,\n",
    "    'invalid_pct': dist_negative/total*100,\n",
    "    'threshold': '< 0'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'distance',\n",
    "    'rule': 'too_short',\n",
    "    'invalid_count': dist_too_short,\n",
    "    'invalid_pct': dist_too_short/total*100,\n",
    "    'threshold': f'< {DISTANCE_MIN}mi'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'distance',\n",
    "    'rule': 'exceeds_max',\n",
    "    'invalid_count': dist_exceeds_max,\n",
    "    'invalid_pct': dist_exceeds_max/total*100,\n",
    "    'threshold': f'> {DISTANCE_MAX}mi'\n",
    "})\n",
    "\n",
    "# Fare rows\n",
    "report_data.append({\n",
    "    'field': 'fare',\n",
    "    'rule': 'null',\n",
    "    'invalid_count': fare_null,\n",
    "    'invalid_pct': fare_null/total*100,\n",
    "    'threshold': 'IS NULL'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'fare',\n",
    "    'rule': 'negative',\n",
    "    'invalid_count': fare_negative,\n",
    "    'invalid_pct': fare_negative/total*100,\n",
    "    'threshold': '< 0'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'fare',\n",
    "    'rule': 'zero',\n",
    "    'invalid_count': fare_zero,\n",
    "    'invalid_pct': fare_zero/total*100,\n",
    "    'threshold': '= 0'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'fare',\n",
    "    'rule': 'extreme_high',\n",
    "    'invalid_count': fare_extreme_high,\n",
    "    'invalid_pct': fare_extreme_high/total*100,\n",
    "    'threshold': f'> ${FARE_MAX}'\n",
    "})\n",
    "\n",
    "# Overall totals\n",
    "report_data.append({\n",
    "    'field': 'OVERALL',\n",
    "    'rule': 'VALID',\n",
    "    'invalid_count': valid,\n",
    "    'invalid_pct': valid/total*100,\n",
    "    'threshold': 'All checks pass'\n",
    "})\n",
    "report_data.append({\n",
    "    'field': 'OVERALL',\n",
    "    'rule': 'INVALID',\n",
    "    'invalid_count': invalid,\n",
    "    'invalid_pct': invalid/total*100,\n",
    "    'threshold': 'Any check fails'\n",
    "})\n",
    "\n",
    "# Create DataFrame and add metadata\n",
    "report = pd.DataFrame(report_data)\n",
    "report['total_records'] = total\n",
    "report['processing_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Reorder columns for better readability\n",
    "report = report[['field', 'rule', 'threshold', 'invalid_count', 'invalid_pct', 'total_records', 'processing_date']]\n",
    "\n",
    "# Save report\n",
    "report.to_csv(REPORTS_DIR / \"validation_report.csv\", index=False)\n",
    "print(f\"Validation report saved: {REPORTS_DIR / 'validation_report.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete\n"
     ]
    }
   ],
   "source": [
    "# Close DuckDB connection\n",
    "con.close()\n",
    "print(\" Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Results & Data Quality Assessment\n",
    "\n",
    "**Pipeline Execution Summary**\n",
    "\n",
    "This validation pipeline processed 684,376,551 NYC rideshare trip records from 2022-2024, implementing 13 data quality checks across the duration, distance, and fare fields. The analysis achieved 99.91% data quality (683,780,462 valid records) with only 596,089 records (0.09%) flagged for quality issues. Processing completed in approximately 70 minutes.\n",
    "\n",
    "**Field-Specific Findings**\n",
    "\n",
    "**Duration (trip_time):** 99.99% pass rate with 60,285 flagged records. Primary issues were trips under 60 seconds (60,163 records) indicating quick cancellations or GPS timing errors, and only 57 trips exceeding 12 hours.\n",
    "\n",
    "**Distance (trip_miles):** 99.96% pass rate with 256,071 flagged records. Most issues were trips under 0.1 miles (251,660 records) representing GPS noise, with only 4,411 trips exceeding 200 miles.\n",
    "\n",
    "**Fare (base_passenger_fare):** 99.95% pass rate with 333,891 flagged records. Notable findings include 320,373 negative fares (0.047%) warranting investigation, 109,855 zero-fare trips (0.016%) likely representing promotions, and 13,518 fares exceeding $500.\n",
    "\n",
    "**Missing Data Patterns**\n",
    "\n",
    "Three columns show significant nulls but don't impact demand forecasting:\n",
    "- `originating_base_num` (27% null) - Lyft doesn't report this field\n",
    "- `on_scene_datetime` (27% null) - optional driver tracking\n",
    "- `airport_fee` (18% null) - only applies to airport trips\n",
    "\n",
    "**Validation Approach**\n",
    "\n",
    "The flag-based validation strategy preserves all records while marking quality issues through 13 granular flags plus a master validity indicator. This approach enables both comprehensive quality monitoring and flexible dataset creation for different analytical purposes. Zero-fare trips were retained as they represent legitimate demand despite zero payment, while negative fares were excluded as they likely represent refunds or cancellations rather than completed trips.\n",
    "\n",
    "**Output Files**\n",
    "\n",
    "Three datasets created for downstream analysis:\n",
    "- `fhvhv_all_data_flagged.parquet` - All 684M records with validation flags for quality monitoring\n",
    "- `fhvhv_valid_data_for_eda.parquet` - 683.8M valid records (24 original columns) for analysis\n",
    "- `validation_report.csv` - Detailed metrics for all 13 validation rules\n",
    "\n",
    "**Next Steps**\n",
    "\n",
    "Proceed to **02_exploratory_analysis.ipynb** to:\n",
    "- Aggregate trips by borough and time period\n",
    "- Analyze demand patterns (daily, weekly, seasonal)\n",
    "- Engineer features for forecasting models\n",
    "\n",
    "The validation framework established is reusable for future forecasting projects with minimal modification (update thresholds and field names)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
