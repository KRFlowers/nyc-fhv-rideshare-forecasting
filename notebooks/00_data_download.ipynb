{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### HVFHV Data Download Pipeline - Stage 0\n",
    "\n",
    " **Pipeline Position:** Stage 0 of 4\n",
    " ```\n",
    " - Stage 0: Data Download ← **Current**\n",
    " - Stage 1: Data Validation\n",
    " - Stage 2: Exploratory Analysis\n",
    " - Stage 3: Modeling\n",
    " ```\n",
    "\n",
    "**Overview**\n",
    " \n",
    " This notebook downloads NYC High-Volume For-Hire Vehicle (HVFHV) trip data from NYC Open Data.\n",
    " HVFHV includes app-based rideshare services: Uber, Lyft, Via, and Juno.\n",
    "\n",
    "**What This Notebook Does**\n",
    "\n",
    " 1. Downloads monthly HVFHV Parquet files from NYC Open Data (2020-2024)\n",
    " 2. Combines files using DuckDB (memory-efficient, faster than pandas)\n",
    " 3. Exports combined data in Parquet and CSV formats\n",
    "\n",
    "**Dataset**\n",
    "\n",
    " - Source: NYC TLC HVFHV data (Uber, Lyft, Via)  \n",
    " - Files: `fhvhv_tripdata_*.parquet`  \n",
    " - Output: `data/final/combined_hvfhv_tripdata.parquet`\n",
    "\n",
    "**Next Step**\n",
    "\n",
    "  - After data is loaded, run `01_data_validation.ipynb` to validate data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Configuration\n",
    " All constants and configuration parameters are defined here for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Dataset: FHVHV (High-Volume FHV - Uber/Lyft/Via/Juno)\n",
      "  Years: 2020-2024 (5 years)\n",
      "  Months per year: 12\n",
      "  Total files: 60\n",
      "  Output base name: combined_fhvhv_tripdata\n",
      "\n",
      "  Using DuckDB for memory-efficient processing\n"
     ]
    }
   ],
   "source": [
    "# === Configuration ===\n",
    "DATA_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "# Full dataset: 2020-2024 (5 years, 60 months)\n",
    "YEARS = [2020, 2021, 2022, 2023, 2024]\n",
    "\n",
    "# Directory structure\n",
    "raw_folder = DATA_ROOT / \"data\" / \"raw\"\n",
    "final_folder = DATA_ROOT / \"data\" / \"final\"\n",
    "\n",
    "# External resources\n",
    "BASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_TYPE = \"fhvhv\"  # High-Volume For-Hire Vehicle (Uber/Lyft)\n",
    "OUTPUT_FILENAME = f\"combined_{DATASET_TYPE}_tripdata\"\n",
    "\n",
    "# Display configuration\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Dataset: {DATASET_TYPE.upper()} (High-Volume FHV - Uber/Lyft/Via/Juno)\")\n",
    "print(f\"  Years: {YEARS[0]}-{YEARS[-1]} ({len(YEARS)} years)\")\n",
    "print(f\"  Months per year: 12\")\n",
    "print(f\"  Total files: {len(YEARS) * 12}\")\n",
    "print(f\"  Output base name: {OUTPUT_FILENAME}\")\n",
    "print(f\"\\n  Using DuckDB for memory-efficient processing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 1.3 Create Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_folder.mkdir(parents=True, exist_ok=True)\n",
    "final_folder.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 2. Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 2.1 Generate Download Tasks\n",
    " Create a list of filenames to download. URLs and save paths are derived\n",
    " on-demand to avoid redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 60 download tasks\n"
     ]
    }
   ],
   "source": [
    "# Generate download tasks (store only filename)\n",
    "download_tasks = [\n",
    "    {'filename': f\"{DATASET_TYPE}_tripdata_{year}-{month:02d}.parquet\"}\n",
    "    for year in YEARS\n",
    "    for month in range(1, 13)\n",
    "]\n",
    "\n",
    "print(f\"Generated {len(download_tasks)} download tasks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 2.2 Execute Downloads\n",
    " Download each file from NYC Open Data, deriving the URL and save path from the filename.\n",
    " **Note:** Files already downloaded will be skipped automatically.\n",
    " Some future months may not be available yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete: 60 files available\n"
     ]
    }
   ],
   "source": [
    "downloaded_files = []\n",
    "failed_files = []\n",
    "\n",
    "for i, task in enumerate(download_tasks, 1):\n",
    "    filename = task['filename']\n",
    "    url = f\"{BASE_URL}/{filename}\"\n",
    "    save_path = raw_folder / filename\n",
    "    \n",
    "    # Skip existing files\n",
    "    if save_path.exists():\n",
    "        downloaded_files.append(save_path)\n",
    "        continue\n",
    "    \n",
    "    # Download with error handling\n",
    "    try:\n",
    "        print(f\"[{i}/{len(download_tasks)}] {filename}...\", end=\" \")\n",
    "        urllib.request.urlretrieve(url, save_path)\n",
    "        downloaded_files.append(save_path)\n",
    "        print(\"✓\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {str(e)[:50]}\")\n",
    "        failed_files.append(filename)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nDownload complete: {len(downloaded_files)} files available\")\n",
    "if failed_files:\n",
    "    print(f\"Failed: {len(failed_files)} files (may not exist yet)\")\n",
    "    for fname in failed_files[:5]:  # Show first 5\n",
    "        print(f\"  - {fname}\")\n",
    "    if len(failed_files) > 5:\n",
    "        print(f\"  ... and {len(failed_files) - 5} more\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 3. Combine Files with DuckDB\n",
    " Use DuckDB to combine files efficiently without loading everything into memory.\n",
    " This approach scales to any dataset size and is much faster than pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 3.1 Initialize DuckDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DuckDB connection established\n"
     ]
    }
   ],
   "source": [
    "# Create DuckDB connection\n",
    "con = duckdb.connect()\n",
    "print(\"✓ DuckDB connection established\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 3.2 Analyze Available Data\n",
    " Query the downloaded files to understand what data we have before combining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing downloaded files...\n",
      "✓ Total rows across all files: 1,002,283,074\n",
      "✓ Dataset has 24 columns\n",
      "✓ Date range: 2020-01-01 00:00:00 to 2024-12-31 23:59:59\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing downloaded files...\")\n",
    "\n",
    "# Pattern to match all downloaded files\n",
    "parquet_pattern = str(raw_folder / f\"{DATASET_TYPE}_tripdata_*.parquet\")\n",
    "\n",
    "# Query to select all data from all files\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM read_parquet('{parquet_pattern}')\n",
    "\"\"\"\n",
    "\n",
    "# Get total row count (fast - doesn't load data into memory)\n",
    "row_count = con.execute(f\"SELECT COUNT(*) FROM ({query})\").fetchone()[0]\n",
    "print(f\"✓ Total rows across all files: {row_count:,}\")\n",
    "\n",
    "# Get column information\n",
    "columns = con.execute(f\"SELECT * FROM ({query}) LIMIT 0\").description\n",
    "column_names = [col[0] for col in columns]\n",
    "print(f\"✓ Dataset has {len(column_names)} columns\")\n",
    "\n",
    "# Get date range\n",
    "date_range = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        MIN(pickup_datetime) as start_date,\n",
    "        MAX(pickup_datetime) as end_date\n",
    "    FROM ({query})\n",
    "\"\"\").fetchone()\n",
    "print(f\"✓ Date range: {date_range[0]} to {date_range[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 4. Save Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 4.1 Export Combined Dataset with DuckDB\n",
    " Save the combined dataset directly from DuckDB to Parquet and CSV.\n",
    " This is much faster and more memory-efficient than pandas.\n",
    " **Note:** CSV export may take several minutes for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to Parquet: combined_fhvhv_tripdata.parquet...\n"
     ]
    }
   ],
   "source": [
    "# Define output paths\n",
    "parquet_path = final_folder / f\"{OUTPUT_FILENAME}.parquet\"\n",
    "csv_path = final_folder / f\"{OUTPUT_FILENAME}.csv\"\n",
    "\n",
    "# Save to Parquet (fast!)\n",
    "print(f\"Saving to Parquet: {parquet_path.name}...\")\n",
    "con.execute(f\"\"\"\n",
    "    COPY ({query})\n",
    "    TO '{str(parquet_path)}'\n",
    "    (FORMAT PARQUET, COMPRESSION SNAPPY)\n",
    "\"\"\")\n",
    "parquet_size_mb = parquet_path.stat().st_size / 1024**2\n",
    "print(f\"✓ Saved Parquet: {parquet_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 4.2 Pipeline Health Check\n",
    " Verify that the pipeline completed successfully and data has expected structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline complete: 1,002,283,074 rows saved\n",
      "✓ All expected HVFHV columns present\n"
     ]
    }
   ],
   "source": [
    "# Verify expected HVFHV columns are present\n",
    "expected_columns = ['hvfhs_license_num', 'pickup_datetime', 'dropoff_datetime', \n",
    "                   'trip_miles', 'base_passenger_fare']\n",
    "missing_cols = [col for col in expected_columns if col not in column_names]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"⚠️  Missing expected columns: {missing_cols}\")\n",
    "    print(f\"Available columns: {column_names}\")\n",
    "else:\n",
    "    print(f\"✓ Pipeline complete: {row_count:,} rows saved\")\n",
    "    print(f\"✓ All expected HVFHV columns present\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 4.3 Data Summary\n",
    " Comprehensive overview of the combined dataset using DuckDB queries.\n",
    " DuckDB can compute statistics without loading the entire dataset into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.1 Dataset Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATASET OVERVIEW\n",
      "======================================================================\n",
      "\n",
      "Dimensions:\n",
      "  Total rows: 1,002,283,074\n",
      "  Total columns: 24\n",
      "\n",
      "File Size:\n",
      "  Parquet: 28114.2 MB (27.46 GB)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nDimensions:\")\n",
    "print(f\"  Total rows: {row_count:,}\")\n",
    "print(f\"  Total columns: {len(column_names)}\")\n",
    "\n",
    "# File size\n",
    "print(f\"\\nFile Size:\")\n",
    "print(f\"  Parquet: {parquet_size_mb:.1f} MB ({parquet_size_mb / 1024:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2 Date Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date Range:\n",
      "  Start: 2020-01-01 00:00:00\n",
      "  End: 2024-12-31 23:59:59\n",
      "  Span: 1826 days (5.0 years)\n"
     ]
    }
   ],
   "source": [
    "# Date coverage\n",
    "date_stats = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        MIN(pickup_datetime) as start_date,\n",
    "        MAX(pickup_datetime) as end_date,\n",
    "        DATEDIFF('day', MIN(pickup_datetime), MAX(pickup_datetime)) as span_days\n",
    "    FROM ({query})\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"\\nDate Range:\")\n",
    "print(f\"  Start: {date_stats[0]}\")\n",
    "print(f\"  End: {date_stats[1]}\")\n",
    "print(f\"  Span: {date_stats[2]} days ({date_stats[2] / 365:.1f} years)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3 Company Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company Distribution (3 companies):\n",
      "\n",
      "  Uber      :     729,414,181 trips ( 72.8%)\n",
      "  Lyft      :     269,104,518 trips ( 26.8%)\n",
      "  Via       :       3,764,375 trips (  0.4%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query company trip counts from data\n",
    "company_stats = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        hvfhs_license_num as code,\n",
    "        COUNT(*) as trips\n",
    "    FROM ({query})\n",
    "    GROUP BY hvfhs_license_num\n",
    "    ORDER BY trips DESC\n",
    "\"\"\").fetchall()\n",
    "\n",
    "# Company name mapping\n",
    "company_map = {\n",
    "    'HV0002': 'Juno',\n",
    "    'HV0003': 'Uber',\n",
    "    'HV0004': 'Via',\n",
    "    'HV0005': 'Lyft'\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(f\"Company Distribution ({len(company_stats)} companies):\\n\")\n",
    "\n",
    "total_trips = sum(trips for _, trips in company_stats)\n",
    "\n",
    "for code, trips in company_stats:\n",
    "    name = company_map.get(code, code)\n",
    "    pct = (trips / total_trips) * 100\n",
    "    print(f\"  {name:10s}: {trips:>15,} trips ({pct:>5.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.4 Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Metrics:\n",
      "  Total miles:          4,967,337,155\n",
      "  Avg trip distance:             4.96 miles\n",
      "  Total revenue:      $23,393,211,267.39\n",
      "  Avg fare:           $         23.34\n",
      "  Total tips:         $999,829,665.77\n",
      "  Avg tip:            $          1.00\n",
      "  Avg tip percentage:            4.3%\n"
     ]
    }
   ],
   "source": [
    "data_stats = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        SUM(trip_miles) as total_miles,\n",
    "        AVG(trip_miles) as avg_miles,\n",
    "        SUM(base_passenger_fare) as total_revenue,\n",
    "        AVG(base_passenger_fare) as avg_fare,\n",
    "        SUM(tips) as total_tips,\n",
    "        AVG(tips) as avg_tip\n",
    "    FROM ({query})\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(\"Dataset Metrics:\")\n",
    "print(f\"  Total miles:        {data_stats[0]:>15,.0f}\")\n",
    "print(f\"  Avg trip distance:  {data_stats[1]:>15.2f} miles\")\n",
    "print(f\"  Total revenue:      ${data_stats[2]:>14,.2f}\")\n",
    "print(f\"  Avg fare:           ${data_stats[3]:>14.2f}\")\n",
    "print(f\"  Total tips:         ${data_stats[4]:>14,.2f}\")\n",
    "print(f\"  Avg tip:            ${data_stats[5]:>14.2f}\")\n",
    "\n",
    "# Tip percentage\n",
    "if data_stats[3] > 0:\n",
    "    tip_pct = (data_stats[5] / data_stats[3]) * 100\n",
    "    print(f\"  Avg tip percentage: {tip_pct:>14.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.5 Location Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique Locations:\n",
      "  Pickup zones: 263\n",
      "  Dropoff zones: 264\n"
     ]
    }
   ],
   "source": [
    "# Location statistics\n",
    "location_stats = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT PULocationID) as pickup_zones,\n",
    "        COUNT(DISTINCT DOLocationID) as dropoff_zones\n",
    "    FROM ({query})\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"\\nUnique Locations:\")\n",
    "print(f\"  Pickup zones: {location_stats[0]}\")\n",
    "print(f\"  Dropoff zones: {location_stats[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.6 Column Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column inventory\n",
    "print(f\"\\nColumns ({len(column_names)}):\")\n",
    "for i, col in enumerate(column_names, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.7 Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values (Key Columns):\n",
      "  ✓ No missing values in key columns\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Columns to check for missing values\n",
    "columns_to_check = [\n",
    "    'hvfhs_license_num',\n",
    "    'pickup_datetime',\n",
    "    'dropoff_datetime',\n",
    "    'trip_miles',\n",
    "    'base_passenger_fare',\n",
    "    'tips',\n",
    "    'PULocationID',\n",
    "    'DOLocationID'\n",
    "]\n",
    "\n",
    "print(\"Missing Values (Key Columns):\")\n",
    "\n",
    "has_nulls = FalseS\n",
    "for col in columns_to_check:\n",
    "    # Simple query for each column\n",
    "    null_count = con.execute(f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM ({query}) \n",
    "        WHERE {col} IS NULL\n",
    "    \"\"\").fetchone()[0]\n",
    "    \n",
    "    if null_count > 0:\n",
    "        has_nulls = True\n",
    "        pct = (null_count / row_count) * 100\n",
    "        print(f\"  {col:30s}: {null_count:>12,} ({pct:5.2f}%)\")\n",
    "\n",
    "if not has_nulls:\n",
    "    print(\"  ✓ No missing values in key columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.8 Sample Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample records (all columns) in a transposed format so all fields are visible\n",
    "sample_df = con.execute(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM ({query})\n",
    "    LIMIT 4\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Sample Records - Transposed ({len(sample_df.columns)} columns, {len(sample_df)} rows):\\n\")\n",
    "print(sample_df.T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.9 Summary Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ Data summary complete\")\n",
    "print(f\"→ Next step: Run notebooks/01_data_validation.ipynb\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Close DuckDB connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "**Completed Steps**\n",
    "1. Downloaded HVFHV trip data from NYC Open Data (2020-2024)\n",
    "2. Combined monthly files using DuckDB (memory-efficient)\n",
    "3. Exported to Parquet format\n",
    "4. Verified pipeline success\n",
    "5. Generated data summary\n",
    "\n",
    "**Output Files**\n",
    "- `data/raw/fhvhv_tripdata_*.parquet` - Individual monthly files (60 files)\n",
    "- `data/final/combined_hvfhv_tripdata.parquet` - Combined dataset\n",
    "\n",
    "**Why DuckDB?**\n",
    "DuckDB processes larger-than-RAM datasets without loading everything into memory. This enables analysis of the 28 GB dataset on a standard laptop.\n",
    "\n",
    "**Next Step**\n",
    "Run `01_data_validation.ipynb` to validate data quality and flag invalid records."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
